Cyc (/ˈsaɪk/) is the world's longest-lived artificial intelligence project,[citation needed] attempting to assemble a comprehensive ontology and knowledge base that spans the basic concepts and "rules of thumb" about how the world works (think common sense knowledge but focusing more on things that rarely get written down or said, in contrast with facts one might find somewhere on the internet or retrieve via Google or Wikipedia), with the goal of enabling AI applications to perform human-like reasoning and be less "brittle" when confronted with novel situations that were not preconceived.

Douglas Lenat began the project in July, 1984 at MCC, where he was Principal Scientist 1984-1994, and then, since January 1995, has been under active development by the Cycorp company, where he is the CEO.
The need for a massive symbolic artificial intelligence project of this kind was born in the early 1980s out of a large number of experiences early AI researchers had, in the previous 25 years, wherein their AI programs would generate encouraging early results but then fail to "scale up"—fail to cope with novel situations and problems outside the narrow area they were conceived and engineered to cope with. Douglas Lenat and Alan Kay publicized this need,[1][2][3] and organized a meeting at Stanford in 1983 to consider the problem; the back-of-the-envelope calculations by them and colleagues including Marvin Minsky, Allen Newell, Edward Feigenbaum, and John McCarthy indicated that that effort would require between 1000 and 3000 person-years of effort, hence not fit into the standard academic project model. Fortuitously, events within a year of that meeting enabled that Manhattan-Project-sized effort to get underway.

The project was started in July,1984 as the flagship project of the 400-person Microelectronics and Computer Technology Corporation, a research consortium started by two dozen large United States based corporations "to counter a then ominous Japanese effort in AI, the so-called "fifth-generation" project."[4] The US Government reacted to the Fifth Generation threat by passing the National Cooperative Research Act of 1984, which for the first time allowed US companies to "collude" on long-term high-risk high-payoff research, and MCC and Sematech sprang up to take advantage of that ten-year opportunity. MCC's first President and CEO was Bobby Ray Inman, former NSA Director and Central Intelligence Agency deputy director.

The objective of the Cyc project was to codify, in machine-usable form, the millions of pieces of knowledge that compose human common sense.[5] This entailed, along the way, (1) developing an adequately expressive representation language, CycL,[6] (2) developing an ontology spanning all human concepts down to some appropriate level of detail,[7] (3) developing a knowledge base on that ontological framework,[7] comprising all human knowledge about those concepts down to some appropriate level of detail, and (4) developing an inference engine exponentially faster than those used in then-conventional expert systems,[8][9] to be able to infer the same types and depth of conclusions that humans are capable of, given their knowledge of the world.

In slightly more detail:

The CycL representation language started as an extension of RLL[10][11] (the so-called Representation Language Language, developed in 1979-1980 by Professor Douglas Lenat at Stanford University and his graduate student Russell Greiner), but within a few years of the launch of the Cyc project it became clear that even representing a typical news story or novel or advertisement would require more than the expressive power of full first order logic, namely second order predicate calculus ("What is the relationship between rain and water?") and then even higher-level orders of logic including modal logic, reflection (enabling the system to reason about its progress so far, on a problem on which it's working), and context logic (enabling the system to reason explicitly about the contexts in which its various premises and conclusions might hold), non-monotonic logic, and circumscription. So, by 1989,[6] CycL had expanded in expressive power to Higher Order Logic (HOL).
Triplestore representations (which are akin to the Frame -and-slot representation languages of the 1970s from which RLL sprang) are widespread today in AI. It may be useful to cite a few examples that stress or break that type of representation, typical of the examples that forced the Cyc project to move from a triplestore representation to a much more expressive one during the period 1984-1989:[6] English sentences including negations ("Fred does not own a dog"), nested quantifiers ("Every American has a mother" means for-all x there-exists y... but "Every American has a President" means there-exists y such that for-all x...), nested modals such as "The United States believes that Germany wants NATO to avoid pursuing..." and it's even awkward to represent, in a Triplestore, relationships of arity higher than 2, such as "Los Angeles is between San Diego and San Francisco along US101."
The ontology of Cyc terms grew to about 100,000 during the first decade of the project, to 1994, and as of 2017 contains about 1,500,000 terms. The ontology includes:
416,000 collections (types, sorts, natural kinds, which includes both types of things such as Fish and types of actions such as Fishing)
a little over a million individuals representing
42,500 predicates (relations, attributes, fields, properties, functions),
about a million generally well known entities such as TheUnitedStatesOfAmerica, BarackObama, TheSigningOfTheUSDeclarationOfIndependence, etc.
An arbitrarily large number of additional terms are also implicitly present in the Cyc ontology, in the sense that there are term-denoting functions such as CalendarYearFn (when given the argument 2016, it denotes the calendar year 2016), GovernmentFn (when given the argument France it denotes the government of France), Meter (when given the argument 2016, it denotes a distance of 2.016 kilometers), and nestings and compositions of such function-denoting terms.
The Cyc knowledge base of general common-sense rules and assertions involving those ontological terms was largely created by hand axiom-writing; it grew to about 1 million in 1994, and as of 2017 is about 24.5 million and has taken well over 1,000 person-years of effort to construct.
It is important to understand that the Cyc ontological engineers strive to keep those numbers as small as possible, not inflate them, so long as the deductive closure of the knowledge base isn't reduced. Suppose Cyc is told about one billion individual people, animals, etc. Then it could be told 1018 facts of the form "Mickey Mouse is not the same individual as Bullwinkle the Moose". But instead of that, one could tell Cyc 10,000 Linnaean Taxonomy rules followed by just 108 rules of the form "No mouse is a moose". And even more compactly, Cyc could instead just be given those 10,000 Linnaean Taxonomy rules followed by just one rule of the form "For any two Linnaean taxons, if neither is explicitly known to be a supertaxon of the other, then they are disjoint". Those 10,001 assertions have the same deductive closure as the earlier-mentioned 1018 facts.
The Cyc inference engine design separates the epistemological problem (what content should be in the Cyc KB) from the heuristic problem (how Cyc could efficiently infer arguments hundreds of steps deep, in a sea of tens of millions of axioms). To do the former, the CycL language and well-understood logical inference might suffice,but for the latter Cyc used a community-of-agents architecture, where specialized reasoning modules, each with its own data structure and algorithm, "raised their hand" if they could efficiently make progress on any of the currently open sub-problems. At the end of the first decade, 1994, there were 20 such heuristic level (HL) modules;[8] as of 2017 there are over 1,050 HL modules.[12]
Some of these HL modules are very general, such as a module that caches the Kleene Star (transitive closure) of all the commonly-used transitive relations in Cyc's ontology.
Some are domain-specific, such as a chemical equation-balancer. These can be and often are an "escape" to (pointer to) some externally available program or webservice or online database, such as a module to quickly "compute" the current population of a city by knowing where/how to look that up.
CycL has a publicly released specification[13] and dozens of HL modules were described in,[8] but the actual Cyc inference engine code, and the full list of 1000+ HL modules, is Cycorp-proprietary.[3]

The name "Cyc" (from "encyclopedia", pronounced [saɪk] like syke) is a registered trademark owned by Cycorp. Access to Cyc is through paid licenses, but bona fide AI research groups are given research-only no-cost licenses (cf. ResearchCyc); there are currently over 600 such groups worldwide with such licenses.

Typical pieces of knowledge represented in the Cyc knowledge base are "Every tree is a plant" and "Plants die eventually". When asked whether trees die, the inference engine can draw the obvious conclusion and answer the question correctly.

Most of Cyc's knowledge, outside math and games, is only true by default. For example, Cyc knows that as a default parents love their children, when you're made happy you smile, taking your first step is a big accomplishment, when someone you love has a big accomplishment that makes you happy, and only adults have children. When asked whether a picture captioned "Someone watching his daughter take her first step" contains a smiling adult person, Cyc can logically infer that the answer is Yes, and "show its work" by presenting the step by step logical argument using those five pieces of knowledge from its KB. These are formulated in the language CycL, which is based on predicate calculus and has a syntax similar to that of the Lisp programming language.

In 2008, Cyc resources were mapped to many Wikipedia articles,[14] potentially easing connecting with other open datasets like DBpedia and Freebase.

Much of the current work Cyc continues to be knowledge engineering, representing facts about the world by hand, and implementing efficient inference mechanisms on that knowledge. Increasingly, however, work at Cycorp involves giving the Cyc system the ability to communicate with end users in natural language, and to assist with the ongoing knowledge formation process via machine learning and natural language understanding. Another large effort at Cycorp is building a suite of Cyc-powered ontological engineering tools to lower the bar to entry for individuals to contribute to, edit, browse, and query Cyc.

Like many companies, Cycorp has ambitions to use Cyc's natural language processing[15] to parse the entire internet to extract structured data;[16] unlike all others, it is able to call on the Cyc system itself to act as an inductive bias and as an adjudicator of ambiguity, metaphor, and ellipsis.